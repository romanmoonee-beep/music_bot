#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Ñ–∞–π–ª–æ–≤
"""
import asyncio
import sys
import os
import subprocess
import shutil
import gzip
import json
from pathlib import Path
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any, Optional
import boto3
from botocore.exceptions import NoCredentialsError

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ PYTHONPATH
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

from app.core.config import settings
from app.core.logging import get_logger
from app.core.database import get_session
from sqlalchemy import text

logger = get_logger(__name__)


class BackupManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    def __init__(self):
        self.project_root = Path(__file__).resolve().parent.parent
        self.backup_dir = self.project_root / "backups"
        self.backup_dir.mkdir(exist_ok=True)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
        self.max_local_backups = 7  # –•—Ä–∞–Ω–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ 7 –±—ç–∫–∞–ø–æ–≤
        self.compression_level = 6  # –£—Ä–æ–≤–µ–Ω—å —Å–∂–∞—Ç–∏—è gzip
        
        # S3 –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        self.s3_bucket = getattr(settings, 'BACKUP_S3_BUCKET', None)
        self.s3_prefix = getattr(settings, 'BACKUP_S3_PREFIX', 'music-bot-backups/')
        self.s3_client = None
        
        if self.s3_bucket:
            try:
                self.s3_client = boto3.client(
                    's3',
                    aws_access_key_id=getattr(settings, 'AWS_ACCESS_KEY_ID', None),
                    aws_secret_access_key=getattr(settings, 'AWS_SECRET_ACCESS_KEY', None),
                    region_name=getattr(settings, 'AWS_REGION', 'us-east-1')
                )
            except Exception as e:
                logger.warning(f"Failed to initialize S3 client: {e}")
    
    def get_timestamp(self) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ timestamp –¥–ª—è –∏–º–µ–Ω —Ñ–∞–π–ª–æ–≤"""
        return datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
    
    async def create_database_backup(self, compress: bool = True) -> Optional[Path]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –±—ç–∫–∞–ø–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            timestamp = self.get_timestamp()
            backup_name = f"db_backup_{timestamp}.sql"
            backup_path = self.backup_dir / backup_name
            
            logger.info(f"üì¶ Creating database backup: {backup_name}")
            
            # –ö–æ–º–∞–Ω–¥–∞ pg_dump
            dump_cmd = [
                "pg_dump",
                f"--host={settings.POSTGRES_SERVER}",
                f"--port={settings.POSTGRES_PORT}",
                f"--username={settings.POSTGRES_USER}",
                f"--dbname={settings.POSTGRES_DB}",
                "--verbose",
                "--clean",
                "--no-owner",
                "--no-privileges",
                "--inserts",  # –î–ª—è –ª—É—á—à–µ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                f"--file={backup_path}"
            ]
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–æ–ª—å —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è
            env = os.environ.copy()
            env['PGPASSWORD'] = settings.POSTGRES_PASSWORD
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –±—ç–∫–∞–ø
            result = subprocess.run(dump_cmd, env=env, capture_output=True, text=True)
            
            if result.returncode != 0:
                logger.error(f"‚ùå pg_dump failed: {result.stderr}")
                return None
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            await self._add_backup_metadata(backup_path)
            
            # –°–∂–∏–º–∞–µ–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if compress:
                compressed_path = await self._compress_file(backup_path)
                if compressed_path:
                    backup_path.unlink()  # –£–¥–∞–ª—è–µ–º –Ω–µ—Å–∂–∞—Ç—ã–π —Ñ–∞–π–ª
                    backup_path = compressed_path
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
            file_size = backup_path.stat().st_size
            size_mb = file_size / (1024 * 1024)
            
            logger.info(f"‚úÖ Database backup created: {backup_path.name} ({size_mb:.1f} MB)")
            
            return backup_path
            
        except Exception as e:
            logger.error(f"‚ùå Failed to create database backup: {e}")
            return None
    
    async def _add_backup_metadata(self, backup_path: Path):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤ –±—ç–∫–∞–ø"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            async with get_session() as session:
                stats = await self._get_database_stats(session)
            
            # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                "backup_created_at": datetime.now(timezone.utc).isoformat(),
                "database_name": settings.POSTGRES_DB,
                "database_host": settings.POSTGRES_SERVER,
                "environment": settings.ENVIRONMENT,
                "version": settings.VERSION,
                "statistics": stats
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ –Ω–∞—á–∞–ª–æ —Ñ–∞–π–ª–∞
            with open(backup_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            with open(backup_path, 'w', encoding='utf-8') as f:
                f.write(f"-- Backup metadata: {json.dumps(metadata, indent=2)}\n")
                f.write(content)
                
        except Exception as e:
            logger.warning(f"Failed to add metadata: {e}")
    
    async def _get_database_stats(self, session) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            stats = {}
            
            # –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã
            tables = [
                'users', 'tracks', 'playlists', 'playlist_tracks',
                'search_history', 'subscriptions', 'payments', 'analytics_events'
            ]
            
            for table in tables:
                try:
                    result = await session.execute(
                        text(f"SELECT COUNT(*) FROM {table}")
                    )
                    stats[f"{table}_count"] = result.scalar()
                except Exception as e:
                    logger.warning(f"Failed to get count for {table}: {e}")
                    stats[f"{table}_count"] = 0
            
            # –†–∞–∑–º–µ—Ä –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            try:
                result = await session.execute(
                    text("SELECT pg_size_pretty(pg_database_size(current_database()))")
                )
                stats['database_size'] = result.scalar()
            except Exception as e:
                logger.warning(f"Failed to get database size: {e}")
            
            return stats
            
        except Exception as e:
            logger.error(f"Failed to get database stats: {e}")
            return {}
    
    async def _compress_file(self, file_path: Path) -> Optional[Path]:
        """–°–∂–∞—Ç–∏–µ —Ñ–∞–π–ª–∞ —Å –ø–æ–º–æ—â—å—é gzip"""
        try:
            compressed_path = file_path.with_suffix(file_path.suffix + '.gz')
            
            logger.info(f"üóúÔ∏è Compressing backup: {file_path.name}")
            
            with open(file_path, 'rb') as f_in:
                with gzip.open(compressed_path, 'wb', compresslevel=self.compression_level) as f_out:
                    shutil.copyfileobj(f_in, f_out)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä—ã
            original_size = file_path.stat().st_size
            compressed_size = compressed_path.stat().st_size
            compression_ratio = (1 - compressed_size / original_size) * 100
            
            logger.info(f"‚úÖ Compression completed: {compression_ratio:.1f}% reduction")
            
            return compressed_path
            
        except Exception as e:
            logger.error(f"‚ùå Failed to compress file: {e}")
            return None
    
    async def create_files_backup(self, include_logs: bool = False) -> Optional[Path]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –±—ç–∫–∞–ø–∞ —Ñ–∞–π–ª–æ–≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
        try:
            timestamp = self.get_timestamp()
            backup_name = f"files_backup_{timestamp}.tar.gz"
            backup_path = self.backup_dir / backup_name
            
            logger.info(f"üìÅ Creating files backup: {backup_name}")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —á—Ç–æ –±—ç–∫–∞–ø–∏—Ç—å
            include_paths = [
                "app/",
                "scripts/",
                "migrations/",
                "requirements.txt",
                "pyproject.toml",
                ".env.example",
                "README.md"
            ]
            
            if include_logs:
                include_paths.append("logs/")
            
            # –ò—Å–∫–ª—é—á–µ–Ω–∏—è
            exclude_patterns = [
                "__pycache__",
                "*.pyc",
                "*.pyo",
                ".git",
                ".pytest_cache",
                "temp/",
                "backups/",
                ".env"
            ]
            
            # –ö–æ–º–∞–Ω–¥–∞ tar
            tar_cmd = ["tar", "-czf", str(backup_path)]
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è
            for pattern in exclude_patterns:
                tar_cmd.extend(["--exclude", pattern])
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏
            tar_cmd.extend(include_paths)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –≤ –∫–æ—Ä–Ω–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞
            result = subprocess.run(
                tar_cmd,
                cwd=self.project_root,
                capture_output=True,
                text=True
            )
            
            if result.returncode != 0:
                logger.error(f"‚ùå tar failed: {result.stderr}")
                return None
            
            file_size = backup_path.stat().st_size
            size_mb = file_size / (1024 * 1024)
            
            logger.info(f"‚úÖ Files backup created: {backup_path.name} ({size_mb:.1f} MB)")
            
            return backup_path
            
        except Exception as e:
            logger.error(f"‚ùå Failed to create files backup: {e}")
            return None
    
    async def create_full_backup(self, compress_db: bool = True, include_logs: bool = False) -> List[Path]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –±—ç–∫–∞–ø–∞ (–ë–î + —Ñ–∞–π–ª—ã)"""
        try:
            logger.info("üöÄ Starting full backup...")
            
            backups = []
            
            # –ë—ç–∫–∞–ø –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            db_backup = await self.create_database_backup(compress=compress_db)
            if db_backup:
                backups.append(db_backup)
            
            # –ë—ç–∫–∞–ø —Ñ–∞–π–ª–æ–≤
            files_backup = await self.create_files_backup(include_logs=include_logs)
            if files_backup:
                backups.append(files_backup)
            
            if backups:
                total_size = sum(b.stat().st_size for b in backups)
                total_mb = total_size / (1024 * 1024)
                logger.info(f"‚úÖ Full backup completed: {len(backups)} files, {total_mb:.1f} MB total")
            else:
                logger.error("‚ùå Full backup failed: no backups created")
            
            return backups
            
        except Exception as e:
            logger.error(f"‚ùå Full backup failed: {e}")
            return []
    
    async def upload_to_s3(self, backup_path: Path) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –±—ç–∫–∞–ø–∞ –≤ S3"""
        if not self.s3_client or not self.s3_bucket:
            logger.warning("S3 not configured, skipping upload")
            return False
        
        try:
            s3_key = f"{self.s3_prefix}{backup_path.name}"
            
            logger.info(f"‚òÅÔ∏è Uploading to S3: {s3_key}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª
            self.s3_client.upload_file(
                str(backup_path),
                self.s3_bucket,
                s3_key,
                ExtraArgs={
                    'StorageClass': 'STANDARD_IA',  # –î–µ—à–µ–≤–ª–µ –¥–ª—è –±—ç–∫–∞–ø–æ–≤
                    'Metadata': {
                        'created_at': datetime.now(timezone.utc).isoformat(),
                        'environment': settings.ENVIRONMENT,
                        'version': settings.VERSION
                    }
                }
            )
            
            logger.info(f"‚úÖ Uploaded to S3: s3://{self.s3_bucket}/{s3_key}")
            return True
            
        except NoCredentialsError:
            logger.error("‚ùå AWS credentials not found")
            return False
        except Exception as e:
            logger.error(f"‚ùå S3 upload failed: {e}")
            return False
    
    def cleanup_old_backups(self, keep_count: int = None) -> int:
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –ª–æ–∫–∞–ª—å–Ω—ã—Ö –±—ç–∫–∞–ø–æ–≤"""
        try:
            keep_count = keep_count or self.max_local_backups
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –±—ç–∫–∞–ø—ã, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ –¥–∞—Ç–µ
            backups = sorted(
                self.backup_dir.glob("*backup_*.sql*"),
                key=lambda p: p.stat().st_mtime,
                reverse=True
            )
            
            files_backups = sorted(
                self.backup_dir.glob("files_backup_*.tar.gz"),
                key=lambda p: p.stat().st_mtime,
                reverse=True
            )
            
            removed_count = 0
            
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –±—ç–∫–∞–ø—ã –ë–î
            for backup in backups[keep_count:]:
                try:
                    backup.unlink()
                    removed_count += 1
                    logger.info(f"üóëÔ∏è Removed old backup: {backup.name}")
                except Exception as e:
                    logger.warning(f"Failed to remove {backup.name}: {e}")
            
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –±—ç–∫–∞–ø—ã —Ñ–∞–π–ª–æ–≤
            for backup in files_backups[keep_count:]:
                try:
                    backup.unlink()
                    removed_count += 1
                    logger.info(f"üóëÔ∏è Removed old backup: {backup.name}")
                except Exception as e:
                    logger.warning(f"Failed to remove {backup.name}: {e}")
            
            if removed_count > 0:
                logger.info(f"‚úÖ Cleaned up {removed_count} old backup(s)")
            else:
                logger.info("‚ÑπÔ∏è No old backups to clean up")
            
            return removed_count
            
        except Exception as e:
            logger.error(f"‚ùå Failed to cleanup old backups: {e}")
            return 0
    
    async def restore_database(self, backup_path: Path, drop_existing: bool = False) -> bool:
        """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏–∑ –±—ç–∫–∞–ø–∞"""
        try:
            if not backup_path.exists():
                logger.error(f"‚ùå Backup file not found: {backup_path}")
                return False
            
            logger.info(f"üîÑ Restoring database from: {backup_path.name}")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —Å–∂–∞—Ç –ª–∏ —Ñ–∞–π–ª
            if backup_path.suffix == '.gz':
                # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ
                temp_path = backup_path.with_suffix('')
                with gzip.open(backup_path, 'rb') as f_in:
                    with open(temp_path, 'wb') as f_out:
                        shutil.copyfileobj(f_in, f_out)
                restore_path = temp_path
            else:
                restore_path = backup_path
            
            # –ö–æ–º–∞–Ω–¥–∞ psql –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
            restore_cmd = [
                "psql",
                f"--host={settings.POSTGRES_SERVER}",
                f"--port={settings.POSTGRES_PORT}",
                f"--username={settings.POSTGRES_USER}",
                f"--dbname={settings.POSTGRES_DB}",
                "--verbose",
                f"--file={restore_path}"
            ]
            
            # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
            if drop_existing:
                logger.warning("‚ö†Ô∏è Dropping existing data before restore")
                # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å DROP –∫–æ–º–∞–Ω–¥—ã –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å --clean –≤ pg_dump
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–æ–ª—å —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è
            env = os.environ.copy()
            env['PGPASSWORD'] = settings.POSTGRES_PASSWORD
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
            result = subprocess.run(restore_cmd, env=env, capture_output=True, text=True)
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –µ—Å–ª–∏ –±—ã–ª
            if backup_path.suffix == '.gz' and restore_path != backup_path:
                restore_path.unlink()
            
            if result.returncode != 0:
                logger.error(f"‚ùå Database restore failed: {result.stderr}")
                return False
            
            logger.info("‚úÖ Database restored successfully")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to restore database: {e}")
            return False
    
    def list_backups(self) -> Dict[str, List[Dict[str, Any]]]:
        """–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –±—ç–∫–∞–ø–æ–≤"""
        try:
            db_backups = []
            files_backups = []
            
            # –ë—ç–∫–∞–ø—ã –ë–î
            for backup in sorted(self.backup_dir.glob("db_backup_*.sql*"), key=lambda p: p.stat().st_mtime, reverse=True):
                stat = backup.stat()
                db_backups.append({
                    'name': backup.name,
                    'path': str(backup),
                    'size_mb': stat.st_size / (1024 * 1024),
                    'created_at': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                    'compressed': backup.suffix == '.gz'
                })
            
            # –ë—ç–∫–∞–ø—ã —Ñ–∞–π–ª–æ–≤
            for backup in sorted(self.backup_dir.glob("files_backup_*.tar.gz"), key=lambda p: p.stat().st_mtime, reverse=True):
                stat = backup.stat()
                files_backups.append({
                    'name': backup.name,
                    'path': str(backup),
                    'size_mb': stat.st_size / (1024 * 1024),
                    'created_at': datetime.fromtimestamp(stat.st_mtime).isoformat()
                })
            
            return {
                'database_backups': db_backups,
                'files_backups': files_backups
            }
            
        except Exception as e:
            logger.error(f"‚ùå Failed to list backups: {e}")
            return {'database_backups': [], 'files_backups': []}
    
    async def verify_backup(self, backup_path: Path) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –±—ç–∫–∞–ø–∞"""
        try:
            logger.info(f"üîç Verifying backup: {backup_path.name}")
            
            verification = {
                'file_exists': backup_path.exists(),
                'file_size': 0,
                'readable': False,
                'metadata_found': False,
                'sql_syntax_ok': False,
                'estimated_records': 0
            }
            
            if not verification['file_exists']:
                return verification
            
            verification['file_size'] = backup_path.stat().st_size
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∏—Ç–∞–µ–º–æ—Å—Ç—å —Ñ–∞–π–ª–∞
            try:
                if backup_path.suffix == '.gz':
                    with gzip.open(backup_path, 'rt', encoding='utf-8') as f:
                        first_lines = [f.readline() for _ in range(10)]
                else:
                    with open(backup_path, 'r', encoding='utf-8') as f:
                        first_lines = [f.readline() for _ in range(10)]
                
                verification['readable'] = True
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                for line in first_lines:
                    if 'Backup metadata:' in line:
                        verification['metadata_found'] = True
                        break
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º SQL —Å–∏–Ω—Ç–∞–∫—Å–∏—Å (–±–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)
                sql_keywords = ['CREATE', 'INSERT', 'UPDATE', 'DELETE', 'SELECT']
                content = ' '.join(first_lines).upper()
                if any(keyword in content for keyword in sql_keywords):
                    verification['sql_syntax_ok'] = True
                
            except Exception as e:
                logger.warning(f"Failed to read backup file: {e}")
            
            # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø–∏—Å–µ–π
            if verification['readable']:
                try:
                    if backup_path.suffix == '.gz':
                        with gzip.open(backup_path, 'rt', encoding='utf-8') as f:
                            content = f.read()
                    else:
                        with open(backup_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                    
                    insert_count = content.count('INSERT INTO')
                    verification['estimated_records'] = insert_count
                    
                except Exception as e:
                    logger.warning(f"Failed to estimate records: {e}")
            
            # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
            verification['is_valid'] = (
                verification['file_exists'] and
                verification['readable'] and
                verification['sql_syntax_ok'] and
                verification['file_size'] > 1024  # –ú–∏–Ω–∏–º—É–º 1KB
            )
            
            if verification['is_valid']:
                logger.info("‚úÖ Backup verification passed")
            else:
                logger.warning("‚ö†Ô∏è Backup verification failed")
            
            return verification
            
        except Exception as e:
            logger.error(f"‚ùå Backup verification failed: {e}")
            return {'is_valid': False, 'error': str(e)}
    
    async def schedule_backup(self, interval_hours: int = 24, keep_count: int = 7):
        """–ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –±—ç–∫–∞–ø–æ–≤"""
        logger.info(f"‚è∞ Starting backup scheduler: every {interval_hours}h, keep {keep_count} backups")
        
        while True:
            try:
                # –°–æ–∑–¥–∞–µ–º –±—ç–∫–∞–ø
                backups = await self.create_full_backup()
                
                if backups:
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ S3 –µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω
                    for backup in backups:
                        await self.upload_to_s3(backup)
                    
                    # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –±—ç–∫–∞–ø—ã
                    self.cleanup_old_backups(keep_count)
                    
                    logger.info(f"‚úÖ Scheduled backup completed: {len(backups)} files")
                else:
                    logger.error("‚ùå Scheduled backup failed")
                
                # –ñ–¥–µ–º —Å–ª–µ–¥—É—é—â–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
                await asyncio.sleep(interval_hours * 3600)
                
            except KeyboardInterrupt:
                logger.info("üõë Backup scheduler stopped")
                break
            except Exception as e:
                logger.error(f"‚ùå Backup scheduler error: {e}")
                # –ñ–¥–µ–º –º–µ–Ω—å—à–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
                await asyncio.sleep(300)  # 5 –º–∏–Ω—É—Ç


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Database and files backup management")
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –±—ç–∫–∞–ø–æ–≤
    db_parser = subparsers.add_parser('db', help='Create database backup')
    db_parser.add_argument('--no-compress', action='store_true', help='Skip compression')
    
    files_parser = subparsers.add_parser('files', help='Create files backup')
    files_parser.add_argument('--include-logs', action='store_true', help='Include log files')
    
    full_parser = subparsers.add_parser('full', help='Create full backup (DB + files)')
    full_parser.add_argument('--no-compress', action='store_true', help='Skip DB compression')
    full_parser.add_argument('--include-logs', action='store_true', help='Include log files')
    
    # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
    restore_parser = subparsers.add_parser('restore', help='Restore database from backup')
    restore_parser.add_argument('backup_file', help='Backup file path')
    restore_parser.add_argument('--drop-existing', action='store_true', help='Drop existing data')
    
    # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
    list_parser = subparsers.add_parser('list', help='List all backups')
    cleanup_parser = subparsers.add_parser('cleanup', help='Clean up old backups')
    cleanup_parser.add_argument('--keep', type=int, default=7, help='Number of backups to keep')
    
    verify_parser = subparsers.add_parser('verify', help='Verify backup integrity')
    verify_parser.add_argument('backup_file', help='Backup file path')
    
    # –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫
    schedule_parser = subparsers.add_parser('schedule', help='Run backup scheduler')
    schedule_parser.add_argument('--interval', type=int, default=24, help='Backup interval in hours')
    schedule_parser.add_argument('--keep', type=int, default=7, help='Number of backups to keep')
    
    # S3 –æ–ø–µ—Ä–∞—Ü–∏–∏
    s3_parser = subparsers.add_parser('upload', help='Upload backup to S3')
    s3_parser.add_argument('backup_file', help='Backup file path')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    manager = BackupManager()
    
    try:
        if args.command == 'db':
            backup = await manager.create_database_backup(compress=not args.no_compress)
            if backup:
                print(f"‚úÖ Database backup created: {backup}")
            else:
                sys.exit(1)
        
        elif args.command == 'files':
            backup = await manager.create_files_backup(include_logs=args.include_logs)
            if backup:
                print(f"‚úÖ Files backup created: {backup}")
            else:
                sys.exit(1)
        
        elif args.command == 'full':
            backups = await manager.create_full_backup(
                compress_db=not args.no_compress,
                include_logs=args.include_logs
            )
            if backups:
                print(f"‚úÖ Full backup created: {len(backups)} files")
                for backup in backups:
                    print(f"  üìÅ {backup}")
            else:
                sys.exit(1)
        
        elif args.command == 'restore':
            backup_path = Path(args.backup_file)
            success = await manager.restore_database(backup_path, args.drop_existing)
            if success:
                print("‚úÖ Database restored successfully")
            else:
                sys.exit(1)
        
        elif args.command == 'list':
            backups = manager.list_backups()
            
            print("üìä Available backups:\n")
            
            print("üóÑÔ∏è Database backups:")
            for backup in backups['database_backups']:
                print(f"  üìÅ {backup['name']} ({backup['size_mb']:.1f} MB) - {backup['created_at']}")
            
            print(f"\nüìÅ Files backups:")
            for backup in backups['files_backups']:
                print(f"  üì¶ {backup['name']} ({backup['size_mb']:.1f} MB) - {backup['created_at']}")
        
        elif args.command == 'cleanup':
            removed = manager.cleanup_old_backups(args.keep)
            print(f"‚úÖ Cleaned up {removed} old backup(s)")
        
        elif args.command == 'verify':
            backup_path = Path(args.backup_file)
            verification = await manager.verify_backup(backup_path)
            
            print(f"üîç Backup verification results:")
            for key, value in verification.items():
                print(f"  {key}: {value}")
            
            if not verification.get('is_valid', False):
                sys.exit(1)
        
        elif args.command == 'upload':
            backup_path = Path(args.backup_file)
            success = await manager.upload_to_s3(backup_path)
            if success:
                print("‚úÖ Backup uploaded to S3")
            else:
                sys.exit(1)
        
        elif args.command == 'schedule':
            await manager.schedule_backup(args.interval, args.keep)
        
    except KeyboardInterrupt:
        logger.info("üõë Operation cancelled by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"üí• Command failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())